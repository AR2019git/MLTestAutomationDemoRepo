

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Initial Pyspark code &mdash; pyspark-tdd-template 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> pyspark-tdd-template
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Introduction</a></li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html">jobs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="tests.html">tests package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html">dependencies package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyspark-tdd-template</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Initial Pyspark code</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/readme.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <p>—
title: Introduction
—</p>
<p>There are a few good Blog about modularising, packaging and structuring
the data pipelines for Spark jobs. However, the testing part is often
neglected or covered from very top level. Data application testing is
different and a pipeline change in the logic is always prone to breaking
the logic somewhere else.</p>
<p>This project discusses on a template for data pipeline project using
Apache Spark and its Python([PySpark]{.title-ref}) APIs with special
focus on data-pipeline testing. But, before we deep dive, We need to
touch upon how we can structure our code to use of the approach outlined
here. This project covers the following topics:</p>
<ul class="simple">
<li><p>Structuring ETL codes into testable modules.</p></li>
<li><p>Setting up configurations and test data(Testbed).</p></li>
<li><p>Boilerplate pytest style testcases for PySpark jobs.</p></li>
<li><p>Packaging and submitting jobs in the cluster.</p></li>
</ul>
<div class="section" id="initial-pyspark-code">
<h1>Initial Pyspark code<a class="headerlink" href="#initial-pyspark-code" title="Permalink to this headline">¶</a></h1>
<p>Let's start with a poorly constructed Pyspark pipeline. We will apply
the structure in it one step at a time. We will go over the codes so
that at the end, all the pieces will make sense how this approach can
help us to build a TDD data-pipeline.</p>
<p>Let's consider, we have a pipeline that consume files containing
pageviews data and merge it into a final table.</p>
<p>::: {.literalinclude lines=”18-“}
../../jobs/pipeline_wo_modules.py
::</p>
<p>The application can be submitted on spark</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span> <span class="pre">{.}</span>
<span class="pre">$SPARK_HOME/bin/spark-submit</span> <span class="pre">pipeline_wo_modules.py</span>
<span class="pre">`</span></code></p>
<p>Let's now look into modularising the application.</p>
</div>
<div class="section" id="handling-static-configurations">
<h1>Handling static configurations<a class="headerlink" href="#handling-static-configurations" title="Permalink to this headline">¶</a></h1>
<p>If we look closely to the above code, the file paths and other static
configurations are tightly coupled with the code. For local execution we
want to execute the code in isolation and we will avoid the side effects
that can occur from I/O.</p>
<p>Let's decouple the static configurations as a JSON file
[configs/config.json]{.title-ref}.</p>
<p>::: {.literalinclude}
../../configs/config.json
::</p>
</div>
<div class="section" id="handling-spark-environments">
<h1>Handling spark environments<a class="headerlink" href="#handling-spark-environments" title="Permalink to this headline">¶</a></h1>
<p>It is not practical to test and debug Spark jobs by sending them to a
cluster using spark-submit and examining stack traces for clues on what
went wrong. Fortunately we have [Pypi
Pyspark](<a class="reference external" href="https://pypi.org/project/pyspark/">https://pypi.org/project/pyspark/</a>) locally on
[pipenv](<a class="reference external" href="https://docs.pipenv.org">https://docs.pipenv.org</a>)</p>
<p>Our pipeline should only focus on the business transformations. Let's
take out the auxiliary heavy lifting to a separate module. This module
can be reused for all other pipelines that follow a common structure as
suggested in this project.</p>
<p><a href="#id1"><span class="problematic" id="id2">`</span></a>dependencies.job_submitter`{.interpreted-text role=”mod”} takes care of
the following</p>
<ul class="simple">
<li><p>Handles the creation of spark environment.</p></li>
<li><p>Passes static job configuration parameters from
[configs/config.json]{.title-ref} to the job.</p></li>
<li><p>Parses command line arguments to accept dynamic inputs and pass it
to the job.</p></li>
<li><p>Dynamically loads the requested job module and runs it.</p></li>
</ul>
<p>The job itself has to expose a [run]{.title-ref} method.</p>
<p>::: {.literalinclude lines=”22-“}
../../dependencies/job_submitter.py
::</p>
</div>
<div class="section" id="modularize-the-code">
<h1>Modularize the code<a class="headerlink" href="#modularize-the-code" title="Permalink to this headline">¶</a></h1>
<p>Regardless of the complexity of a data-pipeline, this often reduces to
defining a series of Extract, Transform and Load (ETL) jobs.</p>
<p>So, 1st step to test the application is to modularize to address the
below.</p>
<ul class="simple">
<li><p>Segregate the logic into testable modules.</p></li>
<li><p>Separating out the side effects of reading and writing the data.</p></li>
</ul>
<p>Below is our pipeline structure:</p>
<ul class="simple">
<li><p><a href="#id3"><span class="problematic" id="id4">`</span></a>jobs.pipeline.extract`{.interpreted-text role=”meth”} - deals with
reading the input data and return the DataFrames.</p></li>
<li><p><a href="#id5"><span class="problematic" id="id6">`</span></a>jobs.pipeline.transform`{.interpreted-text role=”meth”} - deals
with defining the business logic and produce the final DataFrame.</p></li>
<li><p><a href="#id7"><span class="problematic" id="id8">`</span></a>jobs.pipeline.load`{.interpreted-text role=”meth”} - deals with
saving the final data into the final destination.</p></li>
<li><p><a href="#id9"><span class="problematic" id="id10">`</span></a>jobs.pipeline.run`{.interpreted-text role=”meth”} - acts as the
entry point for the pipeline and collaborate between different parts
of the pipeline.</p></li>
<li><p>We have taken out the schema for the DataFrames in
[ddl/schema.py]{.title-ref}</p></li>
</ul>
<p>There is a really good blog by [Dr. Alex
loannides](<a class="reference external" href="https://alexioannides.com/2019/07/28/best-practices-for-pyspark-etl-projects/">https://alexioannides.com/2019/07/28/best-practices-for-pyspark-etl-projects/</a>)
and [Eran
Campf](<a class="reference external" href="https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f#.wg3iv4kie">https://developerzen.com/best-practices-writing-production-grade-pyspark-jobs-cb688ac4d20f#.wg3iv4kie</a>)
about structuring ETL projects. Here We have a single module pipeline
here with just singleton Extract, Transform and Load methods. Our
overall project structure would look like.</p>
<p><a href="#id11"><span class="problematic" id="id12">``</span></a>` {.console}
root/</p>
<blockquote>
<div><p><a href="#id13"><span class="problematic" id="id14">|</span></a>– configs/
|   <a href="#id15"><span class="problematic" id="id16">|</span></a>– config.json
<a href="#id17"><span class="problematic" id="id18">|</span></a>– dependencies/
|   <a href="#id19"><span class="problematic" id="id20">|</span></a>– job_submitter.py
<a href="#id21"><span class="problematic" id="id22">|</span></a>– ddl/
|   <a href="#id23"><span class="problematic" id="id24">|</span></a>– schema.py
<a href="#id25"><span class="problematic" id="id26">|</span></a>– jobs/
|   <a href="#id27"><span class="problematic" id="id28">|</span></a>– pipeline.py
<a href="#id29"><span class="problematic" id="id30">|</span></a>– tests/
|   <a href="#id31"><span class="problematic" id="id32">|</span></a>– test_data/
|   <a href="#id33"><span class="problematic" id="id34">|</span></a>– | – employees/
|   <a href="#id35"><span class="problematic" id="id36">|</span></a>– | – employees_report/
|   <a href="#id37"><span class="problematic" id="id38">|</span></a>– conftest.py
|   <a href="#id39"><span class="problematic" id="id40">|</span></a>– test_bed.json
|   <a href="#id41"><span class="problematic" id="id42">|</span></a>– test_pipeline.py
|   Makefile
|   Pipfile
|   Pipfile.lock</p>
</div></blockquote>
<p><a href="#id43"><span class="problematic" id="id44">``</span></a><a href="#id45"><span class="problematic" id="id46">`</span></a></p>
<p>Our final code looks like:</p>
<p>::: {.literalinclude lines=”19-“}
../../jobs/pipeline.py
::</p>
</div>
<div class="section" id="testing-setup">
<h1>Testing setup<a class="headerlink" href="#testing-setup" title="Permalink to this headline">¶</a></h1>
<p>Given that we have structured our ETL jobs in testable modules. We can
feed it a small slice of ‘real-world’ production data that has been
persisted locally([tests/test_data]{.title-ref}) and check it against
expected results. We will be using
[pytest](<a class="reference external" href="https://docs.pytest.org/en/stable/">https://docs.pytest.org/en/stable/</a>) style tests for our
pipeline, under the hood we will also leverage few features (i.e. mock)
form [unittest](<a class="reference external" href="https://docs.python.org/3/library/unittest.html">https://docs.python.org/3/library/unittest.html</a>)</p>
<p>Let's look into the different functionality of our
<a href="#id47"><span class="problematic" id="id48">`</span></a>tests.conftest`{.interpreted-text role=”mod”}</p>
<p>The 1st function of it is to start a SparkSession locally for testing.</p>
<p>::: {.literalinclude lines=”72-83”}
../../tests/conftest.py
::</p>
<p>We have an utility method
<a href="#id49"><span class="problematic" id="id50">`</span></a>tests.conftest.SparkETLTests.setup_testbed`{.interpreted-text
role=”meth”} that reads the [Testbed]{.title-ref} configurations to
create the Dataframes in order to test out transform function.</p>
<p>::: {.literalinclude lines=”85-130”}
../../tests/conftest.py
::</p>
<p>Let's now have a look into our testing code for the Transform method.</p>
<p>::: {.literalinclude pyobject=”test_pipeline_transform”}
../../tests/test_pipeline.py
::</p>
<p>As you can see we have made available the a pytest fixture named
[testbed]{.title-ref}. This object stores the DataFrames and configs for
testing in it's member variables. We are passing the DataFrames created
out of the test files and matching the output DataFrame using another
helper function
<a href="#id51"><span class="problematic" id="id52">`</span></a>tests.conftest.SparkETLTests.comapare_dataframes`{.interpreted-text
role=”meth”}</p>
<p>::: {.literalinclude pyobject=”SparkETLTests.comapare_dataframes”}
../../tests/conftest.py
::</p>
<p>Since the I/O operations are already been separated out we can
introspect their calling behaviour using mocks. These mocks are setup in
<a href="#id53"><span class="problematic" id="id54">`</span></a>tests.conftest.SparkETLTests.setup_mocks`{.interpreted-text
role=”meth”}</p>
<p>::: {.literalinclude pyobject=”SparkETLTests.setup_mocks”}
../../tests/conftest.py
::</p>
<p>And the code is tested using below block</p>
<p>::: {.literalinclude lines=”41-65”}
../../tests/test_pipeline.py
::</p>
<p>I have used the generic
[read](<a class="reference external" href="https://spark.apache.org/docs/2.3.0/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.load">https://spark.apache.org/docs/2.3.0/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.load</a>)
and
[write](<a class="reference external" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameWriter.save">https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameWriter.save</a>)
module of spark for these mocks to work.</p>
<p>Now, let's look into the integration testing, We are now able to test
out pipeline by mocking the return value of the I/O operations.</p>
<p>::: {.literalinclude pyobject=”test_run_integration”}
../../tests/test_pipeline.py
::</p>
<p>The idea is to use immutable test files for performing the whole
validation. Methods can be connected in terms of input and expected
output, across different upstream and downstream modules. A proper
regression can be leveraged by using this approach of immutable test
data and plugged into a CICD deployment.</p>
</div>
<div class="section" id="running-the-example-locally">
<h1>Running the example locally<a class="headerlink" href="#running-the-example-locally" title="Permalink to this headline">¶</a></h1>
<p>We use [pipenv](<a class="reference external" href="https://docs.pipenv.org">https://docs.pipenv.org</a>) for managing project
dependencies and Python environments (i.e. virtual environments). All
development and production dependencies are described in the
[Pipfile]{.title-ref}</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span> <span class="pre">{.console}</span>
<span class="pre">pip</span> <span class="pre">install</span> <span class="pre">pipenv</span>
<span class="pre">`</span></code></p>
<p>Additionally, you can have [pyenv](<a class="reference external" href="https://github.com/pyenv/pyenv">https://github.com/pyenv/pyenv</a>) to
have the desired python enviroment.</p>
<p>To execute the example unit test for this project run</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span> <span class="pre">{.console}</span>
<span class="pre">pipenv</span> <span class="pre">run</span> <span class="pre">python</span> <span class="pre">-m</span> <span class="pre">unittest</span> <span class="pre">tests/test_*.py</span>
<span class="pre">`</span></code></p>
</div>
<div class="section" id="deploying-into-production">
<h1>Deploying into production<a class="headerlink" href="#deploying-into-production" title="Permalink to this headline">¶</a></h1>
<p>The project has a build-in Makefile utility to create zipped dependency
and configs and bundle them together</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span> <span class="pre">{.console}</span>
<span class="pre">make</span> <span class="pre">clean</span>
<span class="pre">make</span> <span class="pre">build</span>
<span class="pre">`</span></code></p>
<p>Now you can run the pipeline using below command</p>
<p><code class="docutils literal notranslate"><span class="pre">`</span> <span class="pre">{.console}</span>
<span class="pre">$SPARK_HOME/bin/spark-submit</span> <span class="pre">\</span>
<span class="pre">--py-files</span> <span class="pre">packages.zip</span> <span class="pre">\</span>
<span class="pre">--files</span> <span class="pre">configs/config.json</span> <span class="pre">\</span>
<span class="pre">dependencies/job_submitter.py</span> <span class="pre">--job</span> <span class="pre">pipeline</span> <span class="pre">--conf-file</span> <span class="pre">configs/config.json</span>
<span class="pre">`</span></code></p>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, soyel.alam@ucdconnect.ie

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>