

<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>Introduction &mdash; pyspark-cicd-project 0.0.1 documentation</title>
  

  
  <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="_static/pygments.css" type="text/css" />

  
  
  
  

  
  <!--[if lt IE 9]>
    <script src="_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
    
      <script type="text/javascript" id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
        <script src="_static/jquery.js"></script>
        <script src="_static/underscore.js"></script>
        <script src="_static/doctools.js"></script>
        <script src="_static/language_data.js"></script>
    
    <script type="text/javascript" src="_static/js/theme.js"></script>

    
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Jobs package" href="jobs.html" />
    <link rel="prev" title="Welcome to pyspark-tdd-template’s documentation!" href="index.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="index.html" class="icon icon-home" alt="Documentation Home"> pyspark-cicd-project
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        
        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">Introduction</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#structuring-etl-application">Structuring ETL application</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#handling-static-configurations">Handling static configurations</a></li>
<li class="toctree-l3"><a class="reference internal" href="#handling-spark-environments">Handling spark environments</a></li>
<li class="toctree-l3"><a class="reference internal" href="#modularize-the-code">Modularize the code</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#testbed-setup">Testbed Setup</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#running-the-example-locally">Running the example locally</a></li>
<li class="toctree-l3"><a class="reference internal" href="#building-artifact">Building Artifact</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#cicd">CICD</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="jobs.html"><cite>Jobs</cite> package</a></li>
<li class="toctree-l1"><a class="reference internal" href="tests.html"><cite>Tests</cite> package</a></li>
<li class="toctree-l1"><a class="reference internal" href="dependencies.html"><cite>Dependencies</cite> package</a></li>
</ul>

            
          
        </div>
        
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">pyspark-cicd-project</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html" class="icon icon-home"></a> &raquo;</li>
        
      <li>Introduction</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/intro.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="introduction">
<h1>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h1>
<p>This blog discusses about developing, testing and deploying data pipelines solely on our local workstation.
As a data engineer, on daily basis we face the challenge of not being able to test the functionality of our code.
Unless performing sort of UAT on actual production like data in some UAT/DEV/PreProd environment.
This project discusses on a template for data pipelines using Apache Spark and its Python(<cite>PySpark</cite>) APIs.
We will mainly focus on a testing framework and CICD using a small sample of production like data stored in csv files.
I call this approach as Testbed, which can be further leveraged to a small regression to have the overall confidence on the application before it is deployed.</p>
<p>But, before we deep dive, We need to touch upon the modularising of our application to testable units and avoid side effects.
This project covers the following topics:</p>
<ul class="simple">
<li><p>Structuring ETL application.</p></li>
<li><p>Testbed Setup.</p></li>
<li><p>Unittesting.</p></li>
<li><p>CICD.</p></li>
</ul>
<div class="section" id="structuring-etl-application">
<h2>Structuring ETL application<a class="headerlink" href="#structuring-etl-application" title="Permalink to this headline">¶</a></h2>
<p>Let’s consider, we have a pipeline that consume files containing pageviews data and merge it into a final table.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="sd">&quot;&quot;&quot;</span>
<span class="sd">Incremental file: input/page_views</span>
<span class="sd">        email,pages</span>
<span class="sd">        james@example.com,home</span>
<span class="sd">        james@example.com,about</span>
<span class="sd">        patricia@example.com,home</span>
<span class="sd">Final Table::</span>
<span class="sd">        +-----------------+---------+------------+-----------+</span>
<span class="sd">        |email            |page_view|created_date|last_active|</span>
<span class="sd">        +-----------------+---------+------------+-----------+</span>
<span class="sd">        |james@example.com|10       |2020-01-01  |2020-07-04 |</span>
<span class="sd">        |mary@example.com |100      |2020-02-04  |2020-02-04 |</span>
<span class="sd">        |john@example.com |1        |2020-03-04  |2020-06-04 |</span>
<span class="sd">        +-----------------+---------+------------+-----------+</span>
<span class="sd">&quot;&quot;&quot;</span>

<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>

    <span class="n">page_views</span> <span class="o">=</span> <span class="n">StructType</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">),</span>
            <span class="n">StructField</span><span class="p">(</span><span class="s1">&#39;pages&#39;</span><span class="p">,</span> <span class="n">StringType</span><span class="p">(),</span> <span class="kc">True</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>

    <span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">csv</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/user/stabsumalam/pyspark-tdd-template/input/page_views&#39;</span><span class="p">,</span>
                                       <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
                                       <span class="n">schema</span><span class="o">=</span><span class="n">page_views</span><span class="p">)</span>
    <span class="n">inc_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">prev_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">tableName</span><span class="o">=</span><span class="s1">&#39;stabsumalam_db.user_pageviews&#39;</span><span class="p">)</span>
    <span class="n">prev_df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
    <span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span>
                         <span class="n">select</span><span class="p">([</span><span class="s1">&#39;email&#39;</span><span class="p">,</span>
                                 <span class="n">col</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;page_view&#39;</span><span class="p">),</span>
                                 <span class="n">current_date</span><span class="p">()</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;last_active&#39;</span><span class="p">)</span>
                                 <span class="p">])</span>
                         <span class="p">)</span>

    <span class="n">df_transformed</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">prev_df</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">email</span> <span class="o">==</span> <span class="n">prev_df</span><span class="o">.</span><span class="n">email</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">)</span><span class="o">.</span>
                                 <span class="n">select</span><span class="p">([</span><span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">email</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">email</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">page_view</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">page_view</span><span class="p">,</span>
                                                                                         <span class="n">lit</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span>
                                             <span class="s1">&#39;page_view&#39;</span><span class="p">),</span>
                                         <span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">created_date</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span>
                                             <span class="s1">&#39;created_date&#39;</span><span class="p">),</span>
                                         <span class="n">coalesce</span><span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">,</span> <span class="n">prev_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span>
                                             <span class="s1">&#39;last_active&#39;</span><span class="p">)</span>
                                         <span class="p">])</span>
                                 <span class="p">)</span>

    <span class="n">df_transformed</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/user/stabsumalam/pyspark-tdd-template/output/user_pageviews&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>

    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
<p>The application can be submitted on spark</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>$SPARK_HOME/bin/spark-submit pipeline_wo_modules.py
</pre></div>
</div>
<p>Ok, you have already pointed out many flaws in this code, so let’s address those.
We will briefly cover this part, the idea here is taken from well documented blog  by <a class="reference external" href="https://alexioannides.com/2019/07/28/best-practices-for-pyspark-etl-projects/">Dr. Alex loannides</a> .
Our overall project structure would look like.</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">root/</span>
<span class="go"> |-- configs/</span>
<span class="go"> |   |-- config.json</span>
<span class="go"> |-- dependencies/</span>
<span class="go"> |   |-- job_submitter.py</span>
<span class="go"> |-- ddl/</span>
<span class="go"> |       |-- schema.py</span>
<span class="go"> |-- jobs/</span>
<span class="go"> |   |-- pipeline.py</span>
<span class="go"> |-- tests/</span>
<span class="go"> |   |-- test_data/</span>
<span class="go"> |   |-- | -- employees/</span>
<span class="go"> |   |-- | -- employees_report/</span>
<span class="go"> |   |-- conftest.py</span>
<span class="go"> |   |-- test_bed.json</span>
<span class="go"> |   |-- test_pipeline.py</span>
<span class="go"> |       Dockerfile</span>
<span class="go"> |       Jenkinsfile</span>
<span class="go"> |   Makefile</span>
<span class="go"> |   Pipfile</span>
<span class="go"> |   Pipfile.lock</span>
</pre></div>
</div>
<div class="section" id="handling-static-configurations">
<h3>Handling static configurations<a class="headerlink" href="#handling-static-configurations" title="Permalink to this headline">¶</a></h3>
<p>First flaw you noticed is the file paths and other static configurations are tightly coupled with the code.
Let’s decouple the static configurations in a JSON file <cite>configs/config.json</cite>.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span>
  <span class="s2">&quot;page_views&quot;</span><span class="p">:</span> <span class="s2">&quot;/user/stabsumalam/pyspark-tdd-template/input/page_views&quot;</span><span class="p">,</span>
  <span class="s2">&quot;user_pageviews&quot;</span><span class="p">:</span> <span class="s2">&quot;stabsumalam_db.user_pageviews&quot;</span><span class="p">,</span>
  <span class="s2">&quot;output_path&quot;</span> <span class="p">:</span> <span class="s2">&quot;/user/stabsumalam/pyspark-tdd-template/output/user_pageviews&quot;</span>
<span class="p">}</span>
</pre></div>
</div>
<p>For isolated testing we will now be able to override few of these in later section.</p>
</div>
<div class="section" id="handling-spark-environments">
<h3>Handling spark environments<a class="headerlink" href="#handling-spark-environments" title="Permalink to this headline">¶</a></h3>
<p>It is not practical to test and debug Spark jobs by sending them to a cluster  using spark-submit and examining stack traces for clues on what went wrong.
Our pipeline should only focus on the business transformations.
Fortunately we can use <a class="reference external" href="https://pypi.org/project/pyspark/">Pypi Pyspark</a> along with <a class="reference external" href="https://docs.pipenv.org">pipenv</a> to manage an isolated environment.
More on this on <cite>running the example locally</cite> section
Let’s take out the heavy lifting to a separate module.
This module can be reused for all other pipelines that follow this common structure.</p>
<p><a class="reference internal" href="dependencies.html#module-dependencies.job_submitter" title="dependencies.job_submitter"><code class="xref py py-mod docutils literal notranslate"><span class="pre">dependencies.job_submitter</span></code></a> takes care of the following</p>
<p>Besides handling the spark session, In parses the <cite>configs/config.json</cite> and any dynamic command line arguments then execute the requested job.
The job itself has to expose a <cite>run</cite> method that is covered in the below section.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">create_spark_session</span><span class="p">(</span><span class="n">job_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Create spark session to run the job</span>

<span class="sd">    :param job_name: job name</span>
<span class="sd">    :type job_name: str</span>
<span class="sd">    :return: spark and logger</span>
<span class="sd">    :rtype: Tuple[SparkSession,Log4j]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span> <span class="o">=</span> <span class="n">SparkSession</span><span class="o">.</span><span class="n">builder</span><span class="o">.</span><span class="n">appName</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span><span class="o">.</span><span class="n">enableHiveSupport</span><span class="p">()</span><span class="o">.</span><span class="n">getOrCreate</span><span class="p">()</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s2">&quot;spark.jars.ivy&quot;</span><span class="p">,</span> <span class="s2">&quot;/tmp/.ivy&quot;</span><span class="p">)</span>
    <span class="n">app_id</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;spark.app.id&#39;</span><span class="p">)</span>
    <span class="n">log4j</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">_jvm</span><span class="o">.</span><span class="n">org</span><span class="o">.</span><span class="n">apache</span><span class="o">.</span><span class="n">log4j</span>
    <span class="n">message_prefix</span> <span class="o">=</span> <span class="s1">&#39;&lt;&#39;</span> <span class="o">+</span> <span class="n">job_name</span> <span class="o">+</span> <span class="s1">&#39; &#39;</span> <span class="o">+</span> <span class="n">app_id</span> <span class="o">+</span> <span class="s1">&#39;&gt;&#39;</span>
    <span class="n">logger</span> <span class="o">=</span> <span class="n">log4j</span><span class="o">.</span><span class="n">LogManager</span><span class="o">.</span><span class="n">getLogger</span><span class="p">(</span><span class="n">message_prefix</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">spark</span><span class="p">,</span> <span class="n">logger</span>


<span class="k">def</span> <span class="nf">load_config_file</span><span class="p">(</span><span class="n">file_name</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reads the configs/config.json file and parse as a dictionary</span>

<span class="sd">    :param file_name: name of the config file</span>
<span class="sd">    :return: config dictionary</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
            <span class="n">conf</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">conf</span>

    <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">FileNotFoundError</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">file_name</span><span class="si">}</span><span class="s1"> Not found&#39;</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">parse_job_args</span><span class="p">(</span><span class="n">job_args</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Reads the additional job_args and parse as a dictionary</span>

<span class="sd">    :param job_args: extra job_args i.e. k1=v1 k2=v2</span>
<span class="sd">    :return: config dictionary</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">a</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;=&#39;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">a</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;=&#39;</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">a</span> <span class="ow">in</span> <span class="n">job_args</span><span class="p">}</span>


<span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">description</span><span class="o">=</span><span class="s1">&#39;Job submitter&#39;</span><span class="p">,</span>
                                     <span class="n">usage</span><span class="o">=</span><span class="s1">&#39;--job job_name, --conf-file config_file_name, --job-args k1=v1 k2=v2&#39;</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--job&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;job name&#39;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s1">&#39;job_name&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--conf-file&#39;</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Config file path&#39;</span><span class="p">,</span> <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s1">&#39;--job-args&#39;</span><span class="p">,</span>
                        <span class="n">help</span><span class="o">=</span><span class="s1">&#39;Additional job arguments, It would be made part of config dict&#39;</span><span class="p">,</span>
                        <span class="n">required</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
                        <span class="n">nargs</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">)</span>
    <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">()</span>
    <span class="n">job_name</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">job_name</span>
    <span class="n">spark</span><span class="p">,</span> <span class="n">logger</span> <span class="o">=</span> <span class="n">create_spark_session</span><span class="p">(</span><span class="n">job_name</span><span class="p">)</span>
    <span class="n">config_file</span> <span class="o">=</span> <span class="n">args</span><span class="o">.</span><span class="n">conf_file</span> <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">conf_file</span> <span class="k">else</span> <span class="s1">&#39;configs/config.json&#39;</span>
    <span class="n">config_dict</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">load_config_file</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">args</span><span class="o">.</span><span class="n">job_args</span><span class="p">:</span>
        <span class="n">job_args</span> <span class="o">=</span> <span class="n">parse_job_args</span><span class="p">(</span><span class="n">args</span><span class="o">.</span><span class="n">job_args</span><span class="p">)</span>
        <span class="n">config_dict</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">job_args</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;calling job </span><span class="si">{</span><span class="n">args</span><span class="o">.</span><span class="n">job_name</span><span class="si">}</span><span class="s1">  with </span><span class="si">{</span><span class="n">config_dict</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">job</span> <span class="o">=</span> <span class="n">importlib</span><span class="o">.</span><span class="n">import_module</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;jobs.</span><span class="si">{</span><span class="n">job_name</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
    <span class="n">job</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">spark</span><span class="o">=</span><span class="n">spark</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config_dict</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">)</span>
    <span class="n">spark</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="section" id="modularize-the-code">
<h3>Modularize the code<a class="headerlink" href="#modularize-the-code" title="Permalink to this headline">¶</a></h3>
<p>Regardless of the complexity of a data-pipeline, this often reduces to defining a series of Extract, Transform and Load (ETL) jobs.
Now let’s modularise the code in such a way that Transform is free from side effects(here IO). IO bound Extract and Load can be tested using mocks.</p>
<p>Below is our job structure:</p>
<ul class="simple">
<li><p><a class="reference internal" href="jobs.html#jobs.pipeline.extract" title="jobs.pipeline.extract"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jobs.pipeline.extract()</span></code></a> - deals with reading the input data and return the DataFrames.</p></li>
<li><p><a class="reference internal" href="jobs.html#jobs.pipeline.transform" title="jobs.pipeline.transform"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jobs.pipeline.transform()</span></code></a> - deals with defining the business logic and produce the final DataFrame.</p></li>
<li><p><a class="reference internal" href="jobs.html#jobs.pipeline.load" title="jobs.pipeline.load"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jobs.pipeline.load()</span></code></a> - deals with saving the final data into the final destination.</p></li>
<li><p><a class="reference internal" href="jobs.html#jobs.pipeline.run" title="jobs.pipeline.run"><code class="xref py py-meth docutils literal notranslate"><span class="pre">jobs.pipeline.run()</span></code></a> - acts as the entry point for the pipeline and collaborate between different parts of the pipeline.</p></li>
<li><p>We have taken out the schema for the DataFrames in <cite>ddl/schema.py</cite> so that we can leverage the structure to construct the test data.</p></li>
</ul>
<p>Our final code looks like:</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">extract</span><span class="p">(</span><span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">logger</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">DataFrame</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Read incremental file and historical data and return as DataFrames</span>

<span class="sd">    :param spark: Spark session object.</span>
<span class="sd">    :type spark: SparkSession</span>
<span class="sd">    :param config: job configuration</span>
<span class="sd">    :type config: Dict</span>
<span class="sd">    :param logger: Py4j Logger</span>
<span class="sd">    :rtype logger: Py4j.Logger</span>
<span class="sd">    :return: Spark DataFrames.</span>
<span class="sd">    :rtype: DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;page_views&#39;</span><span class="p">],</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="o">.</span><span class="n">page_views</span><span class="p">)</span>
    <span class="n">prev_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="p">(</span><span class="n">tableName</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;user_pageviews&#39;</span><span class="p">])</span>

    <span class="k">return</span> <span class="n">inc_df</span><span class="p">,</span> <span class="n">prev_df</span>


<span class="k">def</span> <span class="nf">transform</span><span class="p">(</span><span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">prev_df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">logger</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataFrame</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Transform the data for final loading.</span>

<span class="sd">    :param inc_df: Incremental DataFrame.</span>
<span class="sd">    :type inc_df: DataFrame</span>
<span class="sd">    :param prev_df: Final DataFrame.</span>
<span class="sd">    :type prev_df: DataFrame</span>
<span class="sd">    :param config: job configuration</span>
<span class="sd">    :type config: Dict</span>
<span class="sd">    :param logger: Py4j Logger</span>
<span class="sd">    :rtype logger: Py4j.Logger</span>
<span class="sd">    :return: Transformed DataFrame.</span>
<span class="sd">    :rtype: DataFrame</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># calculating the metrics</span>
    <span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span><span class="o">.</span>
                         <span class="n">select</span><span class="p">([</span><span class="s1">&#39;email&#39;</span><span class="p">,</span>
                                 <span class="n">col</span><span class="p">(</span><span class="s1">&#39;count&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;page_view&#39;</span><span class="p">),</span>
                                 <span class="n">lit</span><span class="p">(</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;process_date&#39;</span><span class="p">])</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;last_active&#39;</span><span class="p">)</span>
                                 <span class="p">])</span>
                         <span class="p">)</span>

    <span class="c1"># merging the data with historical records</span>
    <span class="n">df_transformed</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">prev_df</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">email</span> <span class="o">==</span> <span class="n">prev_df</span><span class="o">.</span><span class="n">email</span><span class="p">,</span> <span class="s1">&#39;full&#39;</span><span class="p">)</span><span class="o">.</span>
                                 <span class="n">select</span><span class="p">([</span><span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">email</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">email</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;email&#39;</span><span class="p">),</span>
                                         <span class="p">(</span><span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">page_view</span><span class="p">,</span> <span class="n">lit</span><span class="p">(</span><span class="mi">0</span><span class="p">))</span> <span class="o">+</span> <span class="n">coalesce</span><span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">page_view</span><span class="p">,</span>
                                                                                         <span class="n">lit</span><span class="p">(</span><span class="mi">0</span><span class="p">)))</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span><span class="s1">&#39;page_view&#39;</span><span class="p">),</span>
                                         <span class="n">coalesce</span><span class="p">(</span><span class="n">prev_df</span><span class="o">.</span><span class="n">created_date</span><span class="p">,</span> <span class="n">inc_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span>
                                             <span class="s1">&#39;created_date&#39;</span><span class="p">),</span>
                                         <span class="n">coalesce</span><span class="p">(</span><span class="n">inc_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">,</span> <span class="n">prev_df</span><span class="o">.</span><span class="n">last_active</span><span class="p">)</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="s1">&#39;date&#39;</span><span class="p">)</span><span class="o">.</span><span class="n">alias</span><span class="p">(</span>
                                             <span class="s1">&#39;last_active&#39;</span><span class="p">)</span>
                                         <span class="p">])</span>
                                 <span class="p">)</span>

    <span class="k">return</span> <span class="n">df_transformed</span>


<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="n">df</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">logger</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Write data in final destination</span>

<span class="sd">    :param df: DataFrame to save.</span>
<span class="sd">    :type df: DataFrame</span>
<span class="sd">    :param config: job configuration</span>
<span class="sd">    :type config: Dict</span>
<span class="sd">    :param logger: Py4j Logger</span>
<span class="sd">    :rtype logger: Py4j.Logger</span>
<span class="sd">    :return: True</span>
<span class="sd">    :rtype: bool</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="n">config</span><span class="p">[</span><span class="s1">&#39;output_path&#39;</span><span class="p">],</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>


<span class="k">def</span> <span class="nf">run</span><span class="p">(</span><span class="n">spark</span><span class="p">:</span> <span class="n">SparkSession</span><span class="p">,</span> <span class="n">config</span><span class="p">:</span> <span class="n">Dict</span><span class="p">,</span> <span class="n">logger</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Entry point to the pipeline</span>

<span class="sd">    :param spark: SparkSession object</span>
<span class="sd">    :type spark: SparkSession</span>
<span class="sd">    :param config: job configurations and command lines</span>
<span class="sd">    :param logger: Log4j Logger</span>
<span class="sd">    :type logger: Log4j.Logger</span>
<span class="sd">    :type config: Dict</span>
<span class="sd">    :return: True</span>
<span class="sd">    :rtype: bool</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;pipeline is starting&#39;</span><span class="p">)</span>

    <span class="c1"># execute the pipeline</span>
    <span class="n">inc_data</span><span class="p">,</span> <span class="n">prev_data</span> <span class="o">=</span> <span class="n">extract</span><span class="p">(</span><span class="n">spark</span><span class="o">=</span><span class="n">spark</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">)</span>
    <span class="n">transformed_data</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">inc_df</span><span class="o">=</span><span class="n">inc_data</span><span class="p">,</span> <span class="n">prev_df</span><span class="o">=</span><span class="n">prev_data</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">)</span>
    <span class="n">load</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">logger</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">warn</span><span class="p">(</span><span class="s1">&#39;pipeline is complete&#39;</span><span class="p">)</span>
    <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="testbed-setup">
<h2>Testbed Setup<a class="headerlink" href="#testbed-setup" title="Permalink to this headline">¶</a></h2>
<p>Given that we have structured our ETL jobs in testable modules.
We can now test the IO bound Extract and Load using mock.
In our idempotent Transform function we will feed a small slice(maybe few hundreds would be enough to test the code) of ‘real-world’ production data, locally stored in csv. Then check it against expected results.
We will be using <a class="reference external" href="https://docs.pytest.org/en/stable/">pytest</a> style tests for our pipeline,
under the hood we will also leverage few features (i.e. mock) form <a class="reference external" href="https://docs.python.org/3/library/unittest.html">unittest</a></p>
<p>All the setup and helper functions ate in <a class="reference internal" href="tests.html#module-tests.conftest" title="tests.conftest"><code class="xref py py-mod docutils literal notranslate"><span class="pre">tests.conftest</span></code></a> so that the developer can focus on the testing and need not worry about creating dataframes/test spark session/mocks etc etc.
Let’s look into the different functionality</p>
<p>The 1st function of it is to start a SparkSession locally for testing.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">setUp</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Start Spark, read configs, create the Dataframes and mocks</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">logger</span> <span class="o">=</span> <span class="n">job_submitter</span><span class="o">.</span><span class="n">create_spark_session</span><span class="p">(</span><span class="s1">&#39;test_pipeline&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">job_submitter</span><span class="o">.</span><span class="n">load_config_file</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">config_file</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_testbed</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">setup_mocks</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">tearDown</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Stop Spark</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">teardown_testbed</span><span class="p">()</span>
</pre></div>
</div>
<p>Since the I/O operations are already been separated out we can introspect their calling behaviour using mocks.
These mocks are setup in <a class="reference internal" href="tests.html#tests.conftest.SparkETLTests.setup_mocks" title="tests.conftest.SparkETLTests.setup_mocks"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tests.conftest.SparkETLTests.setup_mocks()</span></code></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="k">def</span> <span class="nf">setup_mocks</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Mocking spark and dataframes to introspect the calling behaviour for unittesting</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">mock_read</span> <span class="o">=</span> <span class="n">create_autospec</span><span class="p">(</span><span class="n">DataFrameReader</span><span class="p">)</span>
        <span class="n">mock_write</span> <span class="o">=</span> <span class="n">create_autospec</span><span class="p">(</span><span class="n">DataFrameWriter</span><span class="p">)</span>
        <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mock_spark</span><span class="p">)</span><span class="o">.</span><span class="n">read</span> <span class="o">=</span> <span class="n">PropertyMock</span><span class="p">(</span><span class="n">return_value</span><span class="o">=</span><span class="n">mock_read</span><span class="p">)</span>
        <span class="nb">type</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mock_df</span><span class="p">)</span><span class="o">.</span><span class="n">write</span> <span class="o">=</span> <span class="n">PropertyMock</span><span class="p">(</span><span class="n">return_value</span><span class="o">=</span><span class="n">mock_write</span><span class="p">)</span>
</pre></div>
</div>
<p>And the code is tested using below block</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
<span class="k">def</span> <span class="nf">test_pipeline_extract</span><span class="p">(</span><span class="n">testbed</span><span class="p">:</span> <span class="n">SparkETLTests</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Test pipeline.extract method using the mocked spark session and introspect the calling pattern\</span>
<span class="sd">    to make sure spark methods were called with intended arguments</span>
<span class="sd">    .. seealso:: :class:`SparkETLTests`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calling the extract method with mocked spark and test config</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">extract</span><span class="p">(</span><span class="n">spark</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">mock_spark</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="c1"># introspecting the spark method call</span>
    <span class="n">testbed</span><span class="o">.</span><span class="n">mock_spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="o">.</span><span class="n">assert_called_once_with</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/user/stabsumalam/pyspark-tdd-template/input/page_views&#39;</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;csv&#39;</span><span class="p">,</span> <span class="n">header</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">schema</span><span class="o">=</span><span class="n">schema</span><span class="o">.</span><span class="n">page_views</span><span class="p">)</span>
    <span class="n">testbed</span><span class="o">.</span><span class="n">mock_spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">table</span><span class="o">.</span><span class="n">assert_called_once_with</span><span class="p">(</span><span class="n">tableName</span><span class="o">=</span><span class="s1">&#39;stabsumalam_db.user_pageviews&#39;</span><span class="p">)</span>
    <span class="n">testbed</span><span class="o">.</span><span class="n">mock_spark</span><span class="o">.</span><span class="n">reset_mock</span><span class="p">()</span>


<span class="k">def</span> <span class="nf">test_pipeline_load</span><span class="p">(</span><span class="n">testbed</span><span class="p">:</span> <span class="n">SparkETLTests</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Test pipeline.load method using the mocked spark session and introspect the calling pattern\</span>
<span class="sd">    to make sure spark methods were called with intended arguments</span>
<span class="sd">    .. seealso:: :class:`SparkETLTests`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># calling the extract method with mocked spark and test config</span>
    <span class="n">pipeline</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">df</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">mock_df</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">)</span>
    <span class="c1"># introspecting the spark method call</span>
    <span class="n">testbed</span><span class="o">.</span><span class="n">mock_df</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">save</span><span class="o">.</span><span class="n">assert_called_once_with</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="s1">&#39;/user/stabsumalam/pyspark-tdd-template/output/user_pageviews&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>As you notice we have made available the a pytest fixture named <cite>testbed</cite> which stores all the required attributes.I have used the generic <a class="reference external" href="https://spark.apache.org/docs/2.3.0/api/python/_modules/pyspark/sql/readwriter.html#DataFrameReader.load">read</a> and <a class="reference external" href="https://spark.apache.org/docs/latest/api/python/_modules/pyspark/sql/readwriter.html#DataFrameWriter.save">write</a> module of spark for these mocks to work.</p>
<p>Now for Transform method we need two dataframes as input/
It has an utility method <a class="reference internal" href="tests.html#tests.conftest.SparkETLTests.setup_testbed" title="tests.conftest.SparkETLTests.setup_testbed"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tests.conftest.SparkETLTests.setup_testbed()</span></code></a> that reads the <cite>Testbed</cite> configurations to create these dataframes as well as the expected dataframe to compare.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>
    <span class="k">def</span> <span class="nf">setup_testbed</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Creates the Dataframes and tables from the test files as mapped in tests/testbed.json, \</span>
<span class="sd">        store those in instance variable named dataframes. \</span>
<span class="sd">        It also enriches the test specific job configurations as per the test_bed.json</span>

<span class="sd">        tests/test_data/page_views.csv</span>
<span class="sd">        email,pages</span>
<span class="sd">        james@example.com,home</span>
<span class="sd">        james@example.com,about</span>
<span class="sd">        patricia@example.com,home</span>

<span class="sd">        ddl/schema.py</span>
<span class="sd">        page_views = StructType(</span>
<span class="sd">        [StructField(&#39;email&#39;, StringType(), True),</span>
<span class="sd">        StructField(&#39;pages&#39;, StringType(), True)])</span>

<span class="sd">        testbed.json</span>
<span class="sd">        {</span>
<span class="sd">        &quot;data&quot;: {</span>
<span class="sd">        &quot;page_views&quot;: { &quot;file&quot;: &quot;tests/test_data/page_views.csv&quot; , &quot;schema&quot;: &quot;page_views&quot;}</span>
<span class="sd">        }</span>
<span class="sd">        }</span>
<span class="sd">        &quot;&quot;&quot;</span>

        <span class="k">try</span><span class="p">:</span>
            <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;tests/test_bed.json&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
                <span class="n">test_bed_conf</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">f</span><span class="p">)</span>
                <span class="n">data_dict</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">test_bed_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;loading test data from testbed&#39;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">df</span><span class="p">,</span> <span class="n">meta</span> <span class="ow">in</span> <span class="n">data_dict</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
                    <span class="n">dataframe</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">read</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;file&#39;</span><span class="p">),</span>
                                                                <span class="n">schema</span><span class="o">=</span><span class="nb">getattr</span><span class="p">(</span><span class="n">schema</span><span class="p">,</span> <span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;schema&#39;</span><span class="p">),</span> <span class="kc">None</span><span class="p">),</span>
                                                                <span class="o">**</span><span class="bp">self</span><span class="o">.</span><span class="n">file_options</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="n">df</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataframe</span>
                    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s1">&#39;.&#39;</span><span class="p">))</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
                        <span class="bp">self</span><span class="o">.</span><span class="n">spark</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;create database if not exists </span><span class="si">{</span><span class="n">df</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot;.&quot;</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
                    <span class="n">dataframe</span><span class="o">.</span><span class="n">write</span><span class="o">.</span><span class="n">saveAsTable</span><span class="p">(</span><span class="n">df</span><span class="p">,</span> <span class="nb">format</span><span class="o">=</span><span class="s1">&#39;hive&#39;</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;overwrite&#39;</span><span class="p">)</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loaded</span><span class="si">{</span><span class="n">df</span><span class="si">}</span><span class="s1"> from </span><span class="si">{</span><span class="n">meta</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;file&quot;</span><span class="p">)</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

                <span class="n">conf</span><span class="p">:</span> <span class="n">Dict</span> <span class="o">=</span> <span class="n">test_bed_conf</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s1">&#39;config&#39;</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">conf</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;loaded test config </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">config</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

        <span class="k">except</span> <span class="ne">FileNotFoundError</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s1">&#39;No test data to cook&#39;</span><span class="p">)</span>
</pre></div>
</div>
<p>The actual code needs to be written to evaluate the behaviour of our transform method.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_pipeline_transform</span><span class="p">(</span><span class="n">testbed</span><span class="p">:</span> <span class="n">SparkETLTests</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Test pipeline.transform method using small chunks of input data and expected output data\</span>
<span class="sd">    to make sure the function is behaving as expected.</span>
<span class="sd">    .. seealso:: :class:`SparkETLTests`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># getting the input dataframes</span>
    <span class="n">inc_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">testbed</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="s1">&#39;page_views&#39;</span><span class="p">]</span>
    <span class="n">prev_df</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">testbed</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="s1">&#39;stabsumalam_db.user_pageviews&#39;</span><span class="p">]</span>
    <span class="c1"># getting the expected dataframe</span>
    <span class="n">expected_data</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">testbed</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="s1">&#39;exp_user_pageviews&#39;</span><span class="p">]</span>
    <span class="c1"># actual data</span>
    <span class="n">transformed_data</span><span class="p">:</span> <span class="n">DataFrame</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">inc_df</span><span class="o">=</span><span class="n">inc_df</span><span class="p">,</span> <span class="n">prev_df</span><span class="o">=</span><span class="n">prev_df</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span>
    <span class="c1"># comparing the actual and expected data</span>
    <span class="n">testbed</span><span class="o">.</span><span class="n">comapare_dataframes</span><span class="p">(</span><span class="n">df1</span><span class="o">=</span><span class="n">transformed_data</span><span class="p">,</span> <span class="n">df2</span><span class="o">=</span><span class="n">expected_data</span><span class="p">)</span>
</pre></div>
</div>
<p>For DataFrame comparision we have another helper function <a class="reference internal" href="tests.html#tests.conftest.SparkETLTests.comapare_dataframes" title="tests.conftest.SparkETLTests.comapare_dataframes"><code class="xref py py-meth docutils literal notranslate"><span class="pre">tests.conftest.SparkETLTests.comapare_dataframes()</span></code></a></p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>    <span class="nd">@classmethod</span>
    <span class="k">def</span> <span class="nf">comapare_dataframes</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">df1</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">df2</span><span class="p">:</span> <span class="n">DataFrame</span><span class="p">,</span> <span class="n">excluded_keys</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">,</span> <span class="nb">str</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="p">[])</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">        Compares 2 DataFrames for exact match\</span>
<span class="sd">        internally it use pandas.testing.assert_frame_equal</span>


<span class="sd">        :param df1: processed data</span>
<span class="sd">        :type df1: DataFrame</span>
<span class="sd">        :param df2: gold standard expected data</span>
<span class="sd">        :type df2: DataFrame</span>
<span class="sd">        :return: True</span>
<span class="sd">        :param excluded_keys: columns to be excluded from comparision, optional</span>
<span class="sd">        :type excluded_keys: Union[List, str, None]</span>
<span class="sd">        :rtype: Boolean</span>
<span class="sd">        :raises: AssertionError Dataframe mismatch</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="n">excluded_keys</span> <span class="o">=</span> <span class="n">excluded_keys</span> <span class="k">if</span> <span class="nb">type</span><span class="p">(</span><span class="n">excluded_keys</span><span class="p">)</span> <span class="o">==</span> <span class="nb">list</span> <span class="k">else</span> <span class="p">[</span><span class="n">excluded_keys</span><span class="p">]</span>
        <span class="n">df1</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="o">*</span><span class="n">excluded_keys</span><span class="p">)</span>
        <span class="n">df2</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="o">*</span><span class="n">excluded_keys</span><span class="p">)</span>
        <span class="n">sort_columns</span> <span class="o">=</span> <span class="p">[</span><span class="n">cols</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">cols</span> <span class="ow">in</span> <span class="n">df1</span><span class="o">.</span><span class="n">dtypes</span><span class="p">]</span>
        <span class="n">df1_sorted</span> <span class="o">=</span> <span class="n">df1</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">sort_columns</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">df2_sorted</span> <span class="o">=</span> <span class="n">df2</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span><span class="o">.</span><span class="n">sort_values</span><span class="p">(</span><span class="n">by</span><span class="o">=</span><span class="n">sort_columns</span><span class="p">,</span> <span class="n">ignore_index</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">assert_frame_equal</span><span class="p">(</span><span class="n">df1_sorted</span><span class="p">,</span> <span class="n">df2_sorted</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">True</span>
</pre></div>
</div>
<p>Now, let’s look into the integration testing, We are now able to test out pipeline by mocking the return value of the I/O operations.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">test_run_integration</span><span class="p">(</span><span class="n">testbed</span><span class="p">:</span> <span class="n">SparkETLTests</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Test pipeline.run method to make sure the integration is working fine\</span>
<span class="sd">    It avoids reading and writing operations by mocking the load and extract method</span>
<span class="sd">    .. seealso:: :class:`SparkETLTests`</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s1">&#39;jobs.pipeline.load&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">mock_load</span><span class="p">:</span>
        <span class="k">with</span> <span class="n">patch</span><span class="p">(</span><span class="s1">&#39;jobs.pipeline.extract&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">mock_extract</span><span class="p">:</span>
            <span class="n">mock_load</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="kc">True</span>
            <span class="n">mock_extract</span><span class="o">.</span><span class="n">return_value</span> <span class="o">=</span> <span class="p">(</span><span class="n">testbed</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="s1">&#39;page_views&#39;</span><span class="p">],</span> <span class="n">testbed</span><span class="o">.</span><span class="n">dataframes</span><span class="p">[</span><span class="s1">&#39;stabsumalam_db.user_pageviews&#39;</span><span class="p">])</span>
            <span class="n">status</span> <span class="o">=</span> <span class="n">pipeline</span><span class="o">.</span><span class="n">run</span><span class="p">(</span><span class="n">spark</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">spark</span><span class="p">,</span> <span class="n">config</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">config</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="n">testbed</span><span class="o">.</span><span class="n">logger</span><span class="p">)</span>
            <span class="n">testbed</span><span class="o">.</span><span class="n">assertTrue</span><span class="p">(</span><span class="n">status</span><span class="p">)</span>
</pre></div>
</div>
<p>The idea is to use immutable test files for performing the whole validation. Methods can be connected in terms of input and expected output, across different upstream and downstream modules.
A proper regression can be leveraged by using this approach of immutable test data. In this case I demonstrated a single pipeline, However we can have many pipelines inside jobs folder all connected in terms of input and expected outputs.
Which we will now implement a CICD on this pipeline in later section.</p>
<div class="section" id="running-the-example-locally">
<h3>Running the example locally<a class="headerlink" href="#running-the-example-locally" title="Permalink to this headline">¶</a></h3>
<p>We use <a class="reference external" href="https://docs.pipenv.org">pipenv</a> for managing project dependencies and Python environments (i.e. virtual environments).
All development and production dependencies are described in the <cite>Pipfile</cite></p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pip install pipenv</span>
</pre></div>
</div>
<p>Additionally, you can have <a class="reference external" href="https://github.com/pyenv/pyenv">pyenv</a> to have the desired python enviroment.</p>
<p>To execute the example unit test for this project run</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">pipenv run python -m unittest tests/test_*.py</span>
</pre></div>
</div>
</div>
<div class="section" id="building-artifact">
<h3>Building Artifact<a class="headerlink" href="#building-artifact" title="Permalink to this headline">¶</a></h3>
<p>The project has a build-in Makefile utility to create zipped dependency and configs and bundle them together in a packages.zip file</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="go">make clean</span>
<span class="go">make build</span>
</pre></div>
</div>
<p>Now you can run the pipeline using below command</p>
<div class="highlight-console notranslate"><div class="highlight"><pre><span></span><span class="gp">$</span>SPARK_HOME/bin/spark-submit <span class="se">\</span>
--py-files packages.zip <span class="se">\</span>
--files configs/config.json <span class="se">\</span>
dependencies/job_submitter.py --job pipeline --conf-file configs/config.json
</pre></div>
</div>
</div>
</div>
<div class="section" id="cicd">
<h2>CICD<a class="headerlink" href="#cicd" title="Permalink to this headline">¶</a></h2>
<p>This section is quite straightforward, We will use a docker based agent for CICD. the image will be build from the DockerFile present inside the project folder.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">FROM</span> <span class="n">ubuntu</span><span class="p">:</span><span class="mf">18.04</span>

<span class="n">ENV</span> <span class="n">DEBIAN_FRONTEND</span> <span class="n">noninteractive</span>
<span class="n">ENV</span> <span class="n">LC_ALL</span> <span class="n">C</span><span class="o">.</span><span class="n">UTF</span><span class="o">-</span><span class="mi">8</span>
<span class="n">ENV</span> <span class="n">LANG</span> <span class="n">C</span><span class="o">.</span><span class="n">UTF</span><span class="o">-</span><span class="mi">8</span>

<span class="n">LABEL</span> <span class="n">maintainer</span><span class="o">=</span><span class="s2">&quot;soyel.alam@ucdconnect.ie&quot;</span>

<span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">update</span> <span class="o">&amp;&amp;</span> \
	<span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="o">-</span><span class="n">y</span> <span class="n">install</span> <span class="n">sudo</span> <span class="nb">zip</span> <span class="n">awscli</span>

<span class="n">RUN</span> <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="o">-</span><span class="n">q</span> <span class="o">-</span><span class="n">y</span> <span class="n">openjdk</span><span class="o">-</span><span class="mi">8</span><span class="o">-</span><span class="n">jdk</span> <span class="o">&amp;&amp;</span> \
    <span class="n">apt</span><span class="o">-</span><span class="n">get</span> <span class="n">install</span> <span class="o">-</span><span class="n">y</span> <span class="n">python3</span><span class="o">-</span><span class="n">pip</span> <span class="n">python3</span><span class="o">.</span><span class="mi">6</span>

<span class="n">RUN</span> <span class="n">pip3</span> <span class="n">install</span> <span class="o">--</span><span class="n">upgrade</span> <span class="n">pip</span> <span class="o">&amp;&amp;</span> \
    <span class="n">pip3</span> <span class="n">install</span> <span class="n">pipenv</span>

<span class="n">ENV</span> <span class="n">JAVA_HOME</span>  <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">lib</span><span class="o">/</span><span class="n">jvm</span><span class="o">/</span><span class="n">java</span><span class="o">-</span><span class="mi">8</span><span class="o">-</span><span class="n">openjdk</span><span class="o">-</span><span class="n">amd64</span>

<span class="n">WORKDIR</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">app</span>

<span class="n">RUN</span> <span class="n">useradd</span> <span class="n">jenkins</span> <span class="o">-</span><span class="n">d</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">app</span> <span class="o">&amp;&amp;</span> <span class="n">echo</span> <span class="s2">&quot;jenkins:jenkins&quot;</span> <span class="o">|</span> <span class="n">chpasswd</span>

<span class="n">RUN</span> <span class="n">chown</span> <span class="o">-</span><span class="n">R</span> <span class="n">jenkins</span><span class="p">:</span><span class="n">jenkins</span> <span class="o">/</span><span class="n">usr</span><span class="o">/</span><span class="n">src</span><span class="o">/</span><span class="n">app</span>
</pre></div>
</div>
<p>The testing step is same as simple as running <cite>pytest</cite>. Once the testing is successful we are uploading our artifact zip to S3.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">pipeline</span> <span class="p">{</span>
  <span class="n">agent</span> <span class="p">{</span><span class="n">dockerfile</span> <span class="p">{</span>
  <span class="n">args</span> <span class="s2">&quot;-u jenkins&quot;</span><span class="p">}</span>
  <span class="p">}</span>
  <span class="n">stages</span> <span class="p">{</span>
    <span class="n">stage</span><span class="p">(</span><span class="s2">&quot;prepare&quot;</span><span class="p">)</span> <span class="p">{</span>
      <span class="n">steps</span> <span class="p">{</span>
        <span class="n">script</span><span class="p">{</span>
        <span class="n">echo</span> <span class="s2">&quot;pipeline template&quot;</span>
        <span class="n">sh</span> <span class="s2">&quot;ls -lart&quot;</span>
        <span class="n">sh</span> <span class="s2">&quot;pipenv install --dev&quot;</span>
        <span class="p">}</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">stage</span><span class="p">(</span><span class="s2">&quot;test&quot;</span><span class="p">){</span>
      <span class="n">steps</span><span class="p">{</span>
        <span class="n">sh</span> <span class="s2">&quot;pipenv run pytest&quot;</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">stage</span><span class="p">(</span><span class="s2">&quot;prepare artifact&quot;</span><span class="p">){</span>
      <span class="n">steps</span><span class="p">{</span>
        <span class="n">sh</span> <span class="s2">&quot;make build&quot;</span>
      <span class="p">}</span>
    <span class="p">}</span>
    <span class="n">stage</span><span class="p">(</span><span class="s2">&quot;publish artifact&quot;</span><span class="p">){</span>
      <span class="n">steps</span><span class="p">{</span>
        <span class="n">sh</span> <span class="s2">&quot;aws s3 cp &quot;</span>
      <span class="p">}</span>
    <span class="p">}</span>
  <span class="p">}</span>
<span class="p">}</span>
</pre></div>
</div>
<p>That’s all for this blog, Thank you.</p>
</div>
</div>


           </div>
           
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="jobs.html" class="btn btn-neutral float-right" title="Jobs package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="index.html" class="btn btn-neutral float-left" title="Welcome to pyspark-tdd-template’s documentation!" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        
        &copy; Copyright 2020, soyel.alam@ucdconnect.ie

    </p>
  </div>
    
    
    
    Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a
    
    <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a>
    
    provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  

  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>